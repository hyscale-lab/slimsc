{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33972b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: QwQ-32B / aqua_rat / sc_32_control (sc_i=32)\n",
      "Processing: QwQ-32B / aqua_rat / sc_64_control (sc_i=64)\n",
      "Processing: QwQ-32B / aqua_rat / sc_8_control (sc_i=8)\n",
      "Processing: QwQ-32B / aqua_rat / sc_2_control (sc_i=2)\n",
      "Processing: QwQ-32B / aqua_rat / sc_16_control (sc_i=16)\n",
      "Processing: QwQ-32B / aime / sc_64_control (sc_i=64)\n",
      "Processing: QwQ-32B / aime / sc_8_control (sc_i=8)\n",
      "Processing: QwQ-32B / aime / sc_2_control (sc_i=2)\n",
      "Processing: QwQ-32B / aime / sc_4_control (sc_i=4)\n",
      "Processing: QwQ-32B / aime / sc_16_control (sc_i=16)\n",
      "Processing: QwQ-32B / gpqa_diamond / sc_32_control (sc_i=32)\n",
      "Processing: QwQ-32B / gpqa_diamond / sc_64_control (sc_i=64)\n",
      "Processing: QwQ-32B / gpqa_diamond / sc_8_control (sc_i=8)\n",
      "Processing: QwQ-32B / gpqa_diamond / sc_2_control (sc_i=2)\n",
      "Processing: QwQ-32B / gpqa_diamond / sc_16_control (sc_i=16)\n",
      "Processing: R1-Distill-Qwen-14B / aqua_rat / sc_32_control (sc_i=32)\n",
      "Processing: R1-Distill-Qwen-14B / aqua_rat / sc_64_control (sc_i=64)\n",
      "Processing: R1-Distill-Qwen-14B / aqua_rat / sc_8_control (sc_i=8)\n",
      "Processing: R1-Distill-Qwen-14B / aqua_rat / sc_2_control (sc_i=2)\n",
      "Processing: R1-Distill-Qwen-14B / aqua_rat / sc_16_control (sc_i=16)\n",
      "Processing: R1-Distill-Qwen-14B / aime / sc_32_control (sc_i=32)\n",
      "Processing: R1-Distill-Qwen-14B / aime / sc_64_control (sc_i=64)\n",
      "Processing: R1-Distill-Qwen-14B / aime / sc_8_control (sc_i=8)\n",
      "Processing: R1-Distill-Qwen-14B / aime / sc_2_control (sc_i=2)\n",
      "Processing: R1-Distill-Qwen-14B / aime / sc_4_control (sc_i=4)\n",
      "Processing: R1-Distill-Qwen-14B / aime / sc_16_control (sc_i=16)\n",
      "Processing: R1-Distill-Qwen-14B / gpqa_diamond / sc_32_control (sc_i=32)\n",
      "Processing: R1-Distill-Qwen-14B / gpqa_diamond / sc_64_control (sc_i=64)\n",
      "Processing: R1-Distill-Qwen-14B / gpqa_diamond / sc_8_control (sc_i=8)\n",
      "Processing: R1-Distill-Qwen-14B / gpqa_diamond / sc_2_control (sc_i=2)\n",
      "Processing: R1-Distill-Qwen-14B / gpqa_diamond / sc_4_control (sc_i=4)\n",
      "Processing: R1-Distill-Qwen-14B / gpqa_diamond / sc_16_control (sc_i=16)\n",
      "\n",
      "Processed Data:\n",
      "             model_name  dataset_name  sc_i  mean_gpu_cache_usage_perc  \\\n",
      "0               QwQ-32B          aime     2                   0.041284   \n",
      "1               QwQ-32B          aime     4                   0.073075   \n",
      "2               QwQ-32B          aime     8                   0.137531   \n",
      "3               QwQ-32B          aime    16                   0.248838   \n",
      "4               QwQ-32B          aime    64                   0.616560   \n",
      "5               QwQ-32B      aqua_rat     2                   0.011263   \n",
      "6               QwQ-32B      aqua_rat     8                   0.036682   \n",
      "7               QwQ-32B      aqua_rat    16                   0.069513   \n",
      "8               QwQ-32B      aqua_rat    32                   0.123724   \n",
      "9               QwQ-32B      aqua_rat    64                   0.207047   \n",
      "10              QwQ-32B  gpqa_diamond     2                   0.026444   \n",
      "11              QwQ-32B  gpqa_diamond     8                   0.088036   \n",
      "12              QwQ-32B  gpqa_diamond    16                   0.159895   \n",
      "13              QwQ-32B  gpqa_diamond    32                   0.274535   \n",
      "14              QwQ-32B  gpqa_diamond    64                   0.444580   \n",
      "15  R1-Distill-Qwen-14B          aime     2                   0.042181   \n",
      "16  R1-Distill-Qwen-14B          aime     4                   0.069960   \n",
      "17  R1-Distill-Qwen-14B          aime     8                   0.126273   \n",
      "18  R1-Distill-Qwen-14B          aime    16                   0.227050   \n",
      "19  R1-Distill-Qwen-14B          aime    32                   0.371254   \n",
      "20  R1-Distill-Qwen-14B          aime    64                   0.257602   \n",
      "21  R1-Distill-Qwen-14B      aqua_rat     2                   0.011158   \n",
      "22  R1-Distill-Qwen-14B      aqua_rat     8                   0.038132   \n",
      "23  R1-Distill-Qwen-14B      aqua_rat    16                   0.071071   \n",
      "24  R1-Distill-Qwen-14B      aqua_rat    32                   0.128753   \n",
      "25  R1-Distill-Qwen-14B      aqua_rat    64                   0.212888   \n",
      "26  R1-Distill-Qwen-14B  gpqa_diamond     2                   0.030382   \n",
      "27  R1-Distill-Qwen-14B  gpqa_diamond     4                   0.057071   \n",
      "28  R1-Distill-Qwen-14B  gpqa_diamond     8                   0.103989   \n",
      "29  R1-Distill-Qwen-14B  gpqa_diamond    16                   0.198077   \n",
      "30  R1-Distill-Qwen-14B  gpqa_diamond    32                   0.352999   \n",
      "31  R1-Distill-Qwen-14B  gpqa_diamond    64                   0.501032   \n",
      "\n",
      "    total_time_s  mean_kv_cache_memory_gib  cost_kv_mem_gib_total_time_s  \\\n",
      "0       15036.30                  3.156785                  4.746637e+04   \n",
      "1       16322.40                  5.587729                  9.120514e+04   \n",
      "2       16095.58                 10.516464                  1.692686e+05   \n",
      "3       22296.90                 19.027635                  4.242573e+05   \n",
      "4       44008.50                 47.145850                  2.074818e+06   \n",
      "5       19428.46                  0.861200                  1.673178e+04   \n",
      "6       23246.08                  2.804958                  6.520428e+04   \n",
      "7       27729.18                  5.315351                  1.473903e+05   \n",
      "8       37297.36                  9.460712                  3.528596e+05   \n",
      "9       53329.84                 15.832021                  8.443192e+05   \n",
      "10      39778.20                  2.022040                  8.043312e+04   \n",
      "11      55612.26                  6.731797                  3.743704e+05   \n",
      "12      67937.76                 12.226541                  8.306438e+05   \n",
      "13      88797.06                 20.992606                  1.864082e+06   \n",
      "14     127898.10                 33.995216                  4.347924e+06   \n",
      "15       7265.70                  1.579187                  1.147390e+04   \n",
      "16       8512.20                  2.619160                  2.229481e+04   \n",
      "17      10743.00                  4.727419                  5.078666e+04   \n",
      "18      13359.00                  8.500288                  1.135553e+05   \n",
      "19      17883.60                 13.899008                  2.485643e+05   \n",
      "20      20322.00                  9.644109                  1.959876e+05   \n",
      "21      11554.46                  0.417731                  4.826657e+03   \n",
      "22      15750.54                  1.427570                  2.248500e+04   \n",
      "23      18115.28                  2.660773                  4.820065e+04   \n",
      "24      22534.88                  4.820243                  1.086236e+05   \n",
      "25      33093.66                  7.970086                  2.637593e+05   \n",
      "26      27133.92                  1.137434                  3.086305e+04   \n",
      "27      31935.42                  2.136619                  6.823383e+04   \n",
      "28      37022.04                  3.893132                  1.441317e+05   \n",
      "29      43150.14                  7.415600                  3.199842e+05   \n",
      "30      57810.06                 13.215584                  7.639937e+05   \n",
      "31     102421.44                 18.757637                  1.921184e+06   \n",
      "\n",
      "    mean_completion_tokens  \n",
      "0                  24106.8  \n",
      "1                  49657.0  \n",
      "2                  97267.8  \n",
      "3                 201692.2  \n",
      "4                 847682.0  \n",
      "5                   6179.7  \n",
      "6                  23283.5  \n",
      "7                  47447.2  \n",
      "8                  95298.8  \n",
      "9                 190880.0  \n",
      "10                 14973.9  \n",
      "11                 59459.7  \n",
      "12                118794.7  \n",
      "13                235412.2  \n",
      "14                483462.3  \n",
      "15                 19280.3  \n",
      "16                 36009.6  \n",
      "17                 74285.0  \n",
      "18                147614.2  \n",
      "19                290216.7  \n",
      "20                563161.3  \n",
      "21                  4629.1  \n",
      "22                 18606.9  \n",
      "23                 37070.3  \n",
      "24                 73753.4  \n",
      "25                147942.8  \n",
      "26                 13146.6  \n",
      "27                 26724.4  \n",
      "28                 52413.7  \n",
      "29                105445.0  \n",
      "30                210324.6  \n",
      "31                415209.2  \n",
      "\n",
      "Generating combined plot for KV Cache Memory-Time Cost...\n",
      "  Combined plot saved to plots/memory_combined.png\n",
      "\n",
      "Generating combined plot for Mean Total Completion Tokens...\n",
      "  Combined plot saved to plots/tokens_combined.png\n",
      "\n",
      "All combined plots generated and saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# Base path for results\n",
    "user = os.environ.get(\"USER\", \"default_user\")\n",
    "BASE_RESULTS_PATH = Path(\"/home/users/ntu/{user}/slimsc/prune/results\".format(user=user))\n",
    "\n",
    "# Model specific configurations\n",
    "MODEL_CONFIGS = {\n",
    "    \"R1-Distill-Qwen-14B\": {\n",
    "        \"total_gpu_memory_gib\": 39.56 * 2,\n",
    "        \"model_weights_gib\": 15.41 * 2,\n",
    "        \"activation_memory_gib\": 2.95,\n",
    "    },\n",
    "    \"QwQ-32B\": {\n",
    "        \"total_gpu_memory_gib\": 39.56 * 4,\n",
    "        \"model_weights_gib\": 13.95 * 4,\n",
    "        \"activation_memory_gib\": 10.15,\n",
    "    }\n",
    "}\n",
    "\n",
    "GPU_MEMORY_UTILIZATION = 0.9\n",
    "PRECOMPUTED_CACHE_USAGE_FILENAME = \"precomputed_mean_gpu_cache_perc.txt\"\n",
    "\n",
    "# Datasets and Models to process\n",
    "# You can extend these lists if needed\n",
    "MODELS_TO_PROCESS = [\"QwQ-32B\", \"R1-Distill-Qwen-14B\"]\n",
    "DATASETS_TO_PROCESS = [\"aqua_rat\", \"gpqa_diamond\", \"aime\"]\n",
    "\n",
    "# Plotting configuration\n",
    "PLOT_OUTPUT_DIR = \"plots\" # Directory to save the plots\n",
    "PLOT_FILENAME_TEMPLATE_MEMORY = \"memory_{dataset_name}.png\"\n",
    "PLOT_FILENAME_TEMPLATE_TOKENS = \"tokens_{dataset_name}.png\"\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=cm.tab10.colors)\n",
    "MODELS_TO_PROCESS = [\"QwQ-32B\", \"R1-Distill-Qwen-14B\"]\n",
    "DATASETS_TO_PROCESS = [\"aqua_rat\", \"aime\", \"gpqa_diamond\"]\n",
    "\n",
    "# Plotting configuration\n",
    "PLOT_OUTPUT_DIR = \"plots\" # Directory to save the plots\n",
    "PLOT_FILENAME_TEMPLATE_MEMORY = \"memory_combined.png\"\n",
    "PLOT_FILENAME_TEMPLATE_TOKENS = \"tokens_combined.png\"\n",
    "\n",
    "# --- Color/Style setup ---\n",
    "# Define a specific color for each model\n",
    "# Using colors from tab10 for distinct base colors\n",
    "model_colors = {\n",
    "    \"QwQ-32B\": cm.tab10(0), # First color of tab10 (blue)\n",
    "    \"R1-Distill-Qwen-14B\": cm.tab10(1), # Second color of tab10 (orange)\n",
    "    # Add more models/colors if needed\n",
    "}\n",
    "\n",
    "# Define a specific linestyle for each dataset\n",
    "dataset_linestyles = {\n",
    "    \"aqua_rat\": \"-\",  # Solid line\n",
    "    \"aime\": \"--\",     # Dashed line\n",
    "    \"gpqa_diamond\": \":\", # Dotted line\n",
    "    # Add more datasets/linestyles if needed\n",
    "}\n",
    "\n",
    "# Define markers per model (keeping markers per model)\n",
    "model_markers = {\n",
    "    \"QwQ-32B\": \"o\", # Circle marker\n",
    "    \"R1-Distill-Qwen-14B\": \"s\", # Square marker\n",
    "    # Add more models/markers if needed\n",
    "}\n",
    "\n",
    "# --- Legend Label Mapping ---\n",
    "# Map internal model names to desired legend prefixes\n",
    "model_legend_map = {\n",
    "    \"R1-Distill-Qwen-14B\": \"R1-Distill\",\n",
    "    \"QwQ-32B\": \"QwQ\",\n",
    "}\n",
    "\n",
    "# Map internal dataset names to desired legend suffixes\n",
    "dataset_legend_map = {\n",
    "    \"aqua_rat\": \"AQuA\",\n",
    "    \"aime\": \"AIME\",\n",
    "    \"gpqa_diamond\": \"GPQA\",\n",
    "}\n",
    "\n",
    "def get_precomputed_mean_gpu_cache_usage_perc(run_path: Path):\n",
    "    \"\"\"\n",
    "    Retrieves the precomputed mean_gpu_cache_usage_perc from the file\n",
    "    saved by the precomputation script.\n",
    "    \"\"\"\n",
    "    precomputed_file_path = run_path / PRECOMPUTED_CACHE_USAGE_FILENAME\n",
    "    if not precomputed_file_path.exists():\n",
    "        warnings.warn(f\"Precomputed file '{PRECOMPUTED_CACHE_USAGE_FILENAME}' not found in {run_path}. \"\n",
    "                      f\"Please run the precomputation script.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(precomputed_file_path, 'r') as f:\n",
    "            value_str = f.read().strip()\n",
    "            return float(value_str)\n",
    "    except (IOError, ValueError) as e:\n",
    "        warnings.warn(f\"Error reading or parsing precomputed file {precomputed_file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_total_time_for_run(run_path: Path):\n",
    "    \"\"\"\n",
    "    Calculates total processing time from aggregated_metrics.json.\n",
    "    \"\"\"\n",
    "    metrics_file = run_path / \"aggregated_metrics.json\"\n",
    "    if not metrics_file.exists():\n",
    "        warnings.warn(f\"aggregated_metrics.json not found in {run_path}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(metrics_file, 'r') as f:\n",
    "            metrics_data = json.load(f)\n",
    "        \n",
    "        mean_duration = float(metrics_data[\"metrics\"][\"mean_processing_duration_sec_per_question\"])\n",
    "        num_questions = int(metrics_data[\"metrics\"][\"num_questions_processed\"])\n",
    "        return mean_duration * num_questions\n",
    "    except (KeyError, ValueError, TypeError) as e:\n",
    "        warnings.warn(f\"Error processing {metrics_file}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def calculate_mean_kv_cache_memory_gib(model_name: str, mean_gpu_cache_usage_perc: float):\n",
    "    \"\"\"\n",
    "    Calculates the mean KV cache memory in GiB based on the formula.\n",
    "    \"\"\"\n",
    "    if model_name not in MODEL_CONFIGS:\n",
    "        warnings.warn(f\"Model {model_name} not found in MODEL_CONFIGS.\")\n",
    "        return None\n",
    "    if mean_gpu_cache_usage_perc is None: # Important check\n",
    "        return None\n",
    "\n",
    "    config = MODEL_CONFIGS[model_name]\n",
    "    \n",
    "    torch_available_memory = (\n",
    "        GPU_MEMORY_UTILIZATION * config[\"total_gpu_memory_gib\"]\n",
    "        - config[\"model_weights_gib\"]\n",
    "        - config[\"activation_memory_gib\"]\n",
    "    )\n",
    "    \n",
    "    if torch_available_memory < 0:\n",
    "        warnings.warn(f\"Calculated torch_available_memory is negative for {model_name}. Check config.\")\n",
    "        torch_available_memory = 0 # Prevent negative memory\n",
    "\n",
    "    mean_kv_mem_gib = torch_available_memory * mean_gpu_cache_usage_perc\n",
    "    return mean_kv_mem_gib\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for model_name in MODELS_TO_PROCESS:\n",
    "    for dataset_name in DATASETS_TO_PROCESS:\n",
    "        dataset_path = BASE_RESULTS_PATH / model_name / dataset_name\n",
    "        if not dataset_path.is_dir():\n",
    "            warnings.warn(f\"Dataset path not found: {dataset_path}\")\n",
    "            continue\n",
    "\n",
    "        # Look for 'sc_X_control' directories directly under the dataset path\n",
    "        sc_run_dirs = [d for d in dataset_path.iterdir() if d.is_dir() and re.match(r\"sc_\\d+_control\", d.name)]\n",
    "\n",
    "        for run_dir in sc_run_dirs:\n",
    "            run_name = run_dir.name \n",
    "            match = re.match(r\"sc_(\\d+)_control\", run_name)\n",
    "            if not match:\n",
    "                warnings.warn(f\"Could not parse sc_i from directory name: {run_name}\")\n",
    "                continue\n",
    "            \n",
    "            sc_i = int(match.group(1))\n",
    "\n",
    "            if sc_i == 1:\n",
    "                continue # Skip sc_1\n",
    "\n",
    "            print(f\"Processing: {model_name} / {dataset_name} / {run_name} (sc_i={sc_i})\")\n",
    "\n",
    "            # 1. Get the precomputed KV cache usage percentage\n",
    "            mean_gpu_cache_usage_perc = get_precomputed_mean_gpu_cache_usage_perc(run_dir)\n",
    "            if mean_gpu_cache_usage_perc is None:\n",
    "                print(f\"  Skipping {run_name} due to missing or invalid precomputed KV cache usage percentage.\")\n",
    "                continue # Skip this run entirely if cache perc is missing\n",
    "\n",
    "            # 2. Get metrics from aggregated_metrics.json\n",
    "            metrics_file = run_dir / \"aggregated_metrics.json\"\n",
    "            total_time_s = None\n",
    "            mean_completion_tokens = None\n",
    "\n",
    "            if not metrics_file.exists():\n",
    "                warnings.warn(f\"aggregated_metrics.json not found in {run_dir}\")\n",
    "                # We proceed if KV cache perc was found, but total_time_s and tokens will be None\n",
    "            else:\n",
    "                try:\n",
    "                    with open(metrics_file, 'r') as f:\n",
    "                        metrics_data = json.load(f)\n",
    "                    \n",
    "                    # Extract time metrics\n",
    "                    mean_duration = float(metrics_data[\"metrics\"][\"mean_processing_duration_sec_per_question\"])\n",
    "                    num_questions = int(metrics_data[\"metrics\"][\"num_questions_processed\"])\n",
    "                    total_time_s = mean_duration * num_questions\n",
    "\n",
    "                    # Extract completion tokens metric (added)\n",
    "                    # Handle potential string value for mean_total_completion_tokens_per_question\n",
    "                    mean_completion_tokens_val = metrics_data[\"metrics\"][\"mean_total_completion_tokens_per_question\"]\n",
    "                    if isinstance(mean_completion_tokens_val, str):\n",
    "                         # Attempt to parse string like \"123.45\"\n",
    "                         mean_completion_tokens = float(mean_completion_tokens_val)\n",
    "                    else:\n",
    "                         # Assume it's already a number (int or float)\n",
    "                         mean_completion_tokens = float(mean_completion_tokens_val)\n",
    "\n",
    "\n",
    "                except (KeyError, ValueError, TypeError, json.JSONDecodeError) as e:\n",
    "                    warnings.warn(f\"Error processing {metrics_file}: {e}\")\n",
    "                    # total_time_s and mean_completion_tokens remain None\n",
    "\n",
    "            # 3. Calculate KV memory and cost ONLY if total_time_s was successfully extracted\n",
    "            mean_kv_mem_gib = None\n",
    "            cost = None\n",
    "            if total_time_s is not None:\n",
    "                mean_kv_mem_gib = calculate_mean_kv_cache_memory_gib(model_name, mean_gpu_cache_usage_perc)\n",
    "                if mean_kv_mem_gib is not None: # Should not be None if perc is not None, but good practice\n",
    "                    cost = mean_kv_mem_gib * total_time_s\n",
    "                else:\n",
    "                     warnings.warn(f\"KV cache memory calculation failed for {model_name} with perc {mean_gpu_cache_usage_perc}\")\n",
    "\n",
    "\n",
    "            # Append results - include mean_completion_tokens\n",
    "            all_results.append({\n",
    "                \"model_name\": model_name,\n",
    "                \"dataset_name\": dataset_name,\n",
    "                \"sc_i\": sc_i,\n",
    "                \"mean_gpu_cache_usage_perc\": mean_gpu_cache_usage_perc, # Still store the precomputed value\n",
    "                \"total_time_s\": total_time_s, # May be None\n",
    "                \"mean_kv_cache_memory_gib\": mean_kv_mem_gib, # May be None\n",
    "                \"cost_kv_mem_gib_total_time_s\": cost, # May be None\n",
    "                \"mean_completion_tokens\": mean_completion_tokens # May be None\n",
    "            })\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Filter out rows where essential data (like total_time_s for cost, or tokens for tokens plot) is missing\n",
    "# For cost plot: need total_time_s and mean_kv_cache_memory_gib (which depends on perc)\n",
    "# For tokens plot: need mean_completion_tokens\n",
    "# We'll handle skipping missing data specifically in the plotting loops.\n",
    "\n",
    "if not results_df.empty:\n",
    "    results_df = results_df.sort_values(by=[\"model_name\", \"dataset_name\", \"sc_i\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nProcessed Data:\")\n",
    "if not results_df.empty:\n",
    "    print(results_df)\n",
    "else:\n",
    "    print(\"No data was processed successfully.\")\n",
    "\n",
    "if results_df.empty:\n",
    "    print(\"No data to plot.\")\n",
    "else:\n",
    "    # Ensure plot output directory exists\n",
    "    plot_output_path = Path(PLOT_OUTPUT_DIR)\n",
    "    plot_output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Get unique models and datasets for consistent mapping\n",
    "    unique_models = sorted(results_df['model_name'].unique())\n",
    "    unique_datasets = sorted(results_df['dataset_name'].unique())\n",
    "    \n",
    "    # Reference the color, linestyle, and marker mappings defined earlier\n",
    "    # model_colors, dataset_linestyles, model_markers\n",
    "    # Reference the new legend mapping dictionaries\n",
    "    # model_legend_map, dataset_legend_map\n",
    "\n",
    "    # Set x-ticks based on all available sc_i values across all data\n",
    "    unique_sc_i_all = sorted(results_df['sc_i'].unique())\n",
    "\n",
    "    # --- Plot 1: KV Cache Memory-Time Cost ---\n",
    "    print(\"\\nGenerating combined plot for KV Cache Memory-Time Cost...\")\n",
    "    # Increase figure size slightly\n",
    "    fig, ax = plt.subplots(figsize=(4, 3))\n",
    "\n",
    "    # Iterate through each unique combination of Model and Dataset\n",
    "    for (model_name, dataset_name), group in results_df.groupby(['model_name', 'dataset_name']):\n",
    "        # Filter for valid cost data within this group\n",
    "        group_cost_filtered = group[group['cost_kv_mem_gib_total_time_s'].notna()].copy()\n",
    "\n",
    "        if not group_cost_filtered.empty:\n",
    "            # Sort by sc_i for proper line plotting\n",
    "            group_cost_filtered = group_cost_filtered.sort_values(by=\"sc_i\")\n",
    "\n",
    "            # Get color based on the model name\n",
    "            line_color = model_colors.get(model_name, 'gray') # Use gray as a fallback\n",
    "\n",
    "            # Get linestyle based on the dataset name\n",
    "            line_style = dataset_linestyles.get(dataset_name, '-') # Use solid line as fallback\n",
    "\n",
    "            # Get marker based on the model name\n",
    "            marker_style = model_markers.get(model_name, 'x') # Use 'x' as fallback\n",
    "            \n",
    "            # --- Generate the desired legend label ---\n",
    "            legend_model_prefix = model_legend_map.get(model_name, model_name) # Fallback to original name\n",
    "            legend_dataset_suffix = dataset_legend_map.get(dataset_name, dataset_name) # Fallback to original name\n",
    "            custom_label = f\"{legend_model_prefix}-{legend_dataset_suffix}\"\n",
    "            # -----------------------------------------\n",
    "\n",
    "\n",
    "            # Plot the line\n",
    "            ax.plot(group_cost_filtered['sc_i'],\n",
    "                    group_cost_filtered['cost_kv_mem_gib_total_time_s'],\n",
    "                    label=custom_label,  # Use the new custom label\n",
    "                    color=line_color,    # Set color by model\n",
    "                    linestyle=line_style,# Set linestyle by dataset\n",
    "                    marker=marker_style, # Set marker by model\n",
    "                    markersize=3)     \n",
    "\n",
    "    ax.set_xlabel(\"Number of Chains\")\n",
    "    ax.set_ylabel(\"Mean KV Cache Memory (GiB) * Total Time (s)\")\n",
    "    ax.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "    # --- Legend Placement & Size ---\n",
    "    # Place legend INSIDE the plot area using loc='best'\n",
    "    ax.legend(title=\"Legend\", loc='best', fontsize=8.5) \n",
    "\n",
    "    if len(unique_sc_i_all) > 0:\n",
    "        ax.set_xticks(unique_sc_i_all)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    filename = PLOT_FILENAME_TEMPLATE_MEMORY\n",
    "    save_path = plot_output_path / filename\n",
    "    try:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        print(f\"  Combined plot saved to {save_path}\")\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Error saving combined plot {save_path}: {e}\")\n",
    "\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "    # --- Plot 2: Mean Total Completion Tokens ---\n",
    "    print(\"\\nGenerating combined plot for Mean Total Completion Tokens...\")\n",
    "    # Increase figure size consistently with the first plot\n",
    "    fig, ax = plt.subplots(figsize=(4, 3)) \n",
    "\n",
    "    # Iterate through each unique combination of Model and Dataset\n",
    "    for (model_name, dataset_name), group in results_df.groupby(['model_name', 'dataset_name']):\n",
    "        # Filter for valid token data within this group\n",
    "        group_tokens_filtered = group[group['mean_completion_tokens'].notna()].copy()\n",
    "\n",
    "        if not group_tokens_filtered.empty:\n",
    "            # Sort by sc_i for proper line plotting\n",
    "            group_tokens_filtered = group_tokens_filtered.sort_values(by=\"sc_i\")\n",
    "\n",
    "            # Get color based on the model name\n",
    "            line_color = model_colors.get(model_name, 'gray')\n",
    "\n",
    "            # Get linestyle based on the dataset name\n",
    "            line_style = dataset_linestyles.get(dataset_name, '-')\n",
    "\n",
    "             # Get marker based on the model name\n",
    "            marker_style = model_markers.get(model_name, 'x')\n",
    "\n",
    "            # --- Generate the desired legend label ---\n",
    "            legend_model_prefix = model_legend_map.get(model_name, model_name) # Fallback to original name\n",
    "            legend_dataset_suffix = dataset_legend_map.get(dataset_name, dataset_name) # Fallback to original name\n",
    "            custom_label = f\"{legend_model_prefix}-{legend_dataset_suffix}\"\n",
    "            # -----------------------------------------\n",
    "\n",
    "            # Plot the line, dividing tokens by 1000\n",
    "            ax.plot(group_tokens_filtered['sc_i'],\n",
    "                    group_tokens_filtered['mean_completion_tokens']/1000,\n",
    "                    label=custom_label,  # Use the new custom label\n",
    "                    color=line_color,    # Set color by model\n",
    "                    linestyle=line_style,# Set linestyle by dataset\n",
    "                    marker=marker_style, # Set marker by model\n",
    "                    markersize=4)\n",
    "\n",
    "    ax.set_xlabel(\"Number of Chains\", fontsize=10) # Reduced font size for better fit\n",
    "    ax.set_ylabel(\"Tokens (Thousands)\", fontsize=10) # Reduced font size for better fit\n",
    "    ax.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "    \n",
    "    # --- Legend Placement & Size ---\n",
    "    # Place legend INSIDE the plot area\n",
    "    ax.legend(title=\"Legend\", loc='best', fontsize=8.5) # Reduced font size for better fit\n",
    "\n",
    "    if len(unique_sc_i_all) > 0:\n",
    "        ax.set_xticks(unique_sc_i_all)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    filename = PLOT_FILENAME_TEMPLATE_TOKENS\n",
    "    save_path = plot_output_path / filename\n",
    "    try:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        print(f\"  Combined plot saved to {save_path}\")\n",
    "    except Exception as e:\n",
    "            warnings.warn(f\"Error saving combined plot {save_path}: {e}\")\n",
    "\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(\"\\nAll combined plots generated and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3323a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
